# Logstash Pipeline Configuration v1.0
# Implements centralized logging with enhanced security and reliability features
# Required plugins:
# - logstash-input-beats v8.9.0
# - logstash-filter-json v3.2.0 
# - logstash-filter-grok v4.4.0
# - logstash-output-elasticsearch v11.0.0

# Pipeline Settings
pipeline {
  workers => "${PIPELINE_WORKERS}"
  batch.size => "${PIPELINE_BATCH_SIZE}" 
  batch.delay => "${PIPELINE_BATCH_DELAY}"
  ordered => true
  safety_after_inactivity => "5s"
}

# Persistent Queue Settings
queue {
  type => "persisted"
  max_bytes => "2gb"
  checkpoint.writes => 1000
}

input {
  # Secure Filebeat Input
  beats {
    port => 5044
    ssl => true
    ssl_certificate => "${SSL_CERT_PATH}"
    ssl_key => "${SSL_KEY_PATH}"
    ssl_verify_mode => "force_peer"
    tags => ["filebeat"]
  }

  # System Logs TCP Input
  tcp {
    port => 5000
    codec => json
    type => "system_logs"
    tags => ["system"]
  }

  # API Logs HTTP Input
  http {
    port => 8080
    codec => json_lines
    type => "api_logs"
    ssl => true
    tags => ["api"]
  }
}

filter {
  # JSON Parsing for Application Logs
  if [type] == "api_logs" {
    json {
      source => "message"
      target => "parsed_json"
      remove_field => ["message"]
      add_field => {
        "environment" => "${ENV:production}"
      }
    }

    # Timestamp Normalization
    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
  }

  # Grok Parsing for System Logs
  if [type] == "system_logs" {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}",
          "%{SYSLOGBASE} %{GREEDYDATA:message}"
        ]
      }
      overwrite => ["message"]
    }

    date {
      match => ["timestamp", "ISO8601"]
      target => "@timestamp"
      remove_field => ["timestamp"]
    }
  }

  # Common Field Processing
  mutate {
    remove_field => ["@version", "host"]
    add_field => {
      "processing_timestamp" => "%{@timestamp}"
      "logstash_instance" => "${HOSTNAME}"
    }
  }

  # Security Filtering
  prune {
    blacklist_names => ["password", "secret", "token", "key"]
  }
}

output {
  # Secure Elasticsearch Output
  elasticsearch {
    hosts => ["${ELASTICSEARCH_HOSTS}"]
    user => "${ELASTICSEARCH_USER}"
    password => "${ELASTICSEARCH_PASSWORD}"
    index => "projectx-logs-%{+YYYY.MM.dd}"
    document_type => "_doc"
    template_name => "projectx-template"
    template_overwrite => true

    # Retry Configuration
    retry_initial_interval => "2s"
    retry_max_interval => "64s"
    retry_on_conflict => 5
    
    # Bulk Operation Settings
    bulk_max_size => 5120
    timeout => "60s"

    # SSL Settings
    ssl => true
    ssl_certificate_verification => true

    # Performance Optimization
    sniffing => true
    healthcheck_enabled => true
  }

  # Dead Letter Queue for Failed Events
  if [@metadata][dead_letter_queue] {
    file {
      path => "/var/log/logstash/dead_letter_queue/%{+YYYY-MM-dd}.log"
      codec => json
    }
  }
}